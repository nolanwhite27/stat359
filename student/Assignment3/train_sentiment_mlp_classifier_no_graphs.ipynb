{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ae7678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Loading Dataset ==========\n",
      "Dataset loaded. Example: {'sentence': ['According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .', 'Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .', 'The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .', 'With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .', \"According to the company 's updated strategy for the years 2009-2012 , Basware targets a long-term net sales growth in the range of 20 % -40 % with an operating profit margin of 10 % -20 % of net sales .\"], 'label': [1, 1, 0, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n========== Loading Dataset ==========\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('financial_phrasebank', 'sentences_50agree', trust_remote_code=True)\n",
    "print(\"Dataset loaded. Example:\", dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695779a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import gensim.downloader as api\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "078b54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94adaee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label\n",
      "0  According to Gran , the company has no plans t...      1\n",
      "1  Technopolis plans to develop in stages an area...      1\n",
      "2  The international electronic industry company ...      0\n",
      "3  With the new production plant the company woul...      2\n",
      "4  According to the company 's updated strategy f...      2\n",
      "label\n",
      "1    2879\n",
      "2    1363\n",
      "0     604\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "print(df.head())\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eceb2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.15,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# create train and validation sets from the remaining data\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.15,\n",
    "    stratify=train_val_df[\"label\"],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4b60ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train distribution:\n",
      "label\n",
      "1    0.594116\n",
      "2    0.281348\n",
      "0    0.124536\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation distribution:\n",
      "label\n",
      "1    0.593851\n",
      "2    0.281553\n",
      "0    0.124595\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test distribution:\n",
      "label\n",
      "1    0.594223\n",
      "2    0.280605\n",
      "0    0.125172\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def print_distribution(name, data):\n",
    "    print(f\"\\n{name} distribution:\")\n",
    "    print(data[\"label\"].value_counts(normalize=True))\n",
    "\n",
    "print_distribution(\"Train\", train_df)\n",
    "print_distribution(\"Validation\", val_df)\n",
    "print_distribution(\"Test\", test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fdca2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train counts: [ 436 2080  985]\n",
      "Class weights: tensor([2.6766, 0.5611, 1.1848])\n"
     ]
    }
   ],
   "source": [
    "y_train = train_df[\"label\"].to_numpy()\n",
    "num_classes = 3\n",
    "\n",
    "counts = np.bincount(y_train, minlength=num_classes)\n",
    "N = counts.sum()\n",
    "\n",
    "class_weights = N / (num_classes * counts)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "print(f\"Train counts: {counts}\")\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72275961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastText Vectors...\n",
      "FastText loaded. Vector size: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading FastText Vectors...\")\n",
    "ft = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "print(\"FastText loaded. Vector size:\", ft.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1888f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?|\\d+(?:\\.\\d+)?\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return TOKEN_RE.findall(text.lower())\n",
    "\n",
    "def mean_pool_fasttext(tokens, ft_vectors, dim=300):\n",
    "    if len(tokens) == 0: # addresses edge case of a sentence w/ 0 tokens\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "\n",
    "    vecs = []\n",
    "    for tok in tokens:\n",
    "        try:\n",
    "            vecs.append(ft_vectors[tok])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "\n",
    "    return np.mean(vecs, axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db23db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)         \n",
    "        self.y = torch.from_numpy(y)          \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "\n",
    "def build_features(df, ft):\n",
    "    X = np.vstack([\n",
    "        mean_pool_fasttext(tokenize(sent), ft)\n",
    "        for sent in df[\"sentence\"]\n",
    "    ])\n",
    "    y = df[\"label\"].values.astype(np.int64)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = build_features(train_df, ft)\n",
    "X_val, y_val = build_features(val_df, ft)\n",
    "X_test, y_test = build_features(test_df, ft)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(NumpyDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(NumpyDataset(X_val, y_val), batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(NumpyDataset(X_test, y_test), batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb662b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the MLP model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=300, hidden_dims=(256, 128), dropout=0.2, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "\n",
    "        layers.append(nn.Linear(prev, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model = MLPClassifier(input_dim=300, hidden_dims=(256, 128), dropout=0.2, num_classes=3).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176c1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a few helper functions to help set up the training loop\n",
    "def accuracy_from_logits(logits, y):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return (preds == y).float().mean().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.detach().cpu().numpy())\n",
    "        all_y.append(y.detach().cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_y = np.concatenate(all_y)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    acc = (all_preds == all_y).mean()\n",
    "    macro_f1 = f1_score(all_y, all_preds, average=\"macro\")\n",
    "\n",
    "    return avg_loss, acc, macro_f1\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.detach().cpu().numpy())\n",
    "        all_y.append(y.detach().cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_y = np.concatenate(all_y)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    acc = (all_preds == all_y).mean()\n",
    "    macro_f1 = f1_score(all_y, all_preds, average=\"macro\")\n",
    "\n",
    "    return avg_loss, acc, macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7d08531",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f055a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training loop\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    num_epochs=30,\n",
    "    save_path=\"best_mlp_fasttext.pt\",\n",
    "):\n",
    "    best_val_f1 = -1.0\n",
    "\n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_macro_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_macro_f1\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "        val_loss, val_acc, val_f1 = evaluate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"train_macro_f1\"].append(train_f1)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_macro_f1\"].append(val_f1)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"train loss {train_loss:.4f} acc {train_acc:.4f} f1 {train_f1:.4f} | \"\n",
    "            f\"val loss {val_loss:.4f} acc {val_acc:.4f} f1 {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved new best model (val macro-F1 = {best_val_f1:.4f}) -> {save_path}\")\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    return best_val_f1, history_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba775444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 1.0710 acc 0.4864 f1 0.4096 | val loss 1.0485 acc 0.4175 f1 0.3525\n",
      "Saved new best model (val macro-F1 = 0.3525) -> best_mlp_fasttext.pt\n",
      "Epoch 02 | train loss 0.9866 acc 0.5338 f1 0.4180 | val loss 0.9603 acc 0.6019 f1 0.4841\n",
      "Saved new best model (val macro-F1 = 0.4841) -> best_mlp_fasttext.pt\n",
      "Epoch 03 | train loss 0.9138 acc 0.5784 f1 0.4938 | val loss 0.8804 acc 0.5858 f1 0.4517\n",
      "Epoch 04 | train loss 0.8543 acc 0.6121 f1 0.5372 | val loss 0.8698 acc 0.6828 f1 0.6015\n",
      "Saved new best model (val macro-F1 = 0.6015) -> best_mlp_fasttext.pt\n",
      "Epoch 05 | train loss 0.8158 acc 0.6170 f1 0.5619 | val loss 0.7935 acc 0.6974 f1 0.6394\n",
      "Saved new best model (val macro-F1 = 0.6394) -> best_mlp_fasttext.pt\n",
      "Epoch 06 | train loss 0.7814 acc 0.6490 f1 0.5942 | val loss 0.7600 acc 0.6424 f1 0.5856\n",
      "Epoch 07 | train loss 0.7308 acc 0.6664 f1 0.6241 | val loss 0.7185 acc 0.7039 f1 0.6518\n",
      "Saved new best model (val macro-F1 = 0.6518) -> best_mlp_fasttext.pt\n",
      "Epoch 08 | train loss 0.7077 acc 0.6755 f1 0.6326 | val loss 0.7137 acc 0.6748 f1 0.6168\n",
      "Epoch 09 | train loss 0.6883 acc 0.6927 f1 0.6488 | val loss 0.6761 acc 0.6926 f1 0.6622\n",
      "Saved new best model (val macro-F1 = 0.6622) -> best_mlp_fasttext.pt\n",
      "Epoch 10 | train loss 0.6664 acc 0.6941 f1 0.6580 | val loss 0.7136 acc 0.6165 f1 0.5982\n",
      "Epoch 11 | train loss 0.6700 acc 0.7007 f1 0.6593 | val loss 0.6687 acc 0.7492 f1 0.7059\n",
      "Saved new best model (val macro-F1 = 0.7059) -> best_mlp_fasttext.pt\n",
      "Epoch 12 | train loss 0.6314 acc 0.7087 f1 0.6710 | val loss 0.6655 acc 0.7638 f1 0.7145\n",
      "Saved new best model (val macro-F1 = 0.7145) -> best_mlp_fasttext.pt\n",
      "Epoch 13 | train loss 0.6229 acc 0.7158 f1 0.6775 | val loss 0.6631 acc 0.7039 f1 0.6818\n",
      "Epoch 14 | train loss 0.5998 acc 0.7232 f1 0.6906 | val loss 0.6553 acc 0.6942 f1 0.6517\n",
      "Epoch 15 | train loss 0.6076 acc 0.7286 f1 0.6936 | val loss 0.6592 acc 0.6893 f1 0.6741\n",
      "Epoch 16 | train loss 0.5861 acc 0.7326 f1 0.7018 | val loss 0.6507 acc 0.6877 f1 0.6590\n",
      "Epoch 17 | train loss 0.5704 acc 0.7421 f1 0.7110 | val loss 0.6637 acc 0.6990 f1 0.6521\n",
      "Epoch 18 | train loss 0.5552 acc 0.7446 f1 0.7094 | val loss 0.6720 acc 0.6667 f1 0.6501\n",
      "Epoch 19 | train loss 0.5493 acc 0.7529 f1 0.7233 | val loss 0.6520 acc 0.7492 f1 0.6990\n",
      "Epoch 20 | train loss 0.5450 acc 0.7555 f1 0.7239 | val loss 0.6566 acc 0.7136 f1 0.6702\n",
      "Epoch 21 | train loss 0.5224 acc 0.7726 f1 0.7429 | val loss 0.6676 acc 0.6796 f1 0.6454\n",
      "Epoch 22 | train loss 0.5136 acc 0.7661 f1 0.7387 | val loss 0.6361 acc 0.7395 f1 0.7060\n",
      "Epoch 23 | train loss 0.5151 acc 0.7669 f1 0.7375 | val loss 0.6417 acc 0.7395 f1 0.6974\n",
      "Epoch 24 | train loss 0.4942 acc 0.7772 f1 0.7491 | val loss 0.6697 acc 0.6618 f1 0.6485\n",
      "Epoch 25 | train loss 0.4759 acc 0.7823 f1 0.7559 | val loss 0.6685 acc 0.7298 f1 0.7045\n",
      "Epoch 26 | train loss 0.4727 acc 0.7881 f1 0.7651 | val loss 0.6446 acc 0.7476 f1 0.6996\n",
      "Epoch 27 | train loss 0.4709 acc 0.7875 f1 0.7611 | val loss 0.6575 acc 0.7395 f1 0.6937\n",
      "Epoch 28 | train loss 0.4516 acc 0.8009 f1 0.7776 | val loss 0.6651 acc 0.7265 f1 0.6936\n",
      "Epoch 29 | train loss 0.4423 acc 0.8052 f1 0.7835 | val loss 0.6763 acc 0.7152 f1 0.6824\n",
      "Epoch 30 | train loss 0.4418 acc 0.7983 f1 0.7742 | val loss 0.6733 acc 0.7476 f1 0.6993\n",
      "    epoch  train_loss  train_acc  train_macro_f1  val_loss   val_acc  \\\n",
      "25     26    0.472654   0.788061        0.765104  0.644643  0.747573   \n",
      "26     27    0.470859   0.787489        0.761074  0.657490  0.739482   \n",
      "27     28    0.451574   0.800914        0.777564  0.665073  0.726537   \n",
      "28     29    0.442270   0.805199        0.783468  0.676340  0.715210   \n",
      "29     30    0.441838   0.798343        0.774151  0.673322  0.747573   \n",
      "\n",
      "    val_macro_f1  \n",
      "25      0.699610  \n",
      "26      0.693694  \n",
      "27      0.693625  \n",
      "28      0.682362  \n",
      "29      0.699338  \n"
     ]
    }
   ],
   "source": [
    "best_val_f1, history_df = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    num_epochs=30,\n",
    "    save_path=\"best_mlp_fasttext.pt\",\n",
    ")\n",
    "\n",
    "history_df.to_csv(\"mlp_fasttext_history.csv\", index=False)\n",
    "print(history_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ea15f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metrics and save plots\n",
    "def plot_metric(history_df, train_col, val_col, ylabel, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(history_df[\"epoch\"], history_df[train_col], label=\"train\")\n",
    "    plt.plot(history_df[\"epoch\"], history_df[val_col], label=\"val\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f\"{ylabel} vs Epochs\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "plot_metric(history_df, \"train_loss\", \"val_loss\", \"Loss\", \"loss_vs_epochs.png\")\n",
    "plot_metric(history_df, \"train_acc\", \"val_acc\", \"Accuracy\", \"accuracy_vs_epochs.png\")\n",
    "plot_metric(history_df, \"train_macro_f1\", \"val_macro_f1\", \"Macro F1\", \"macro_f1_vs_epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "188dc84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Macro-F1: 0.697099742825523\n",
      "MLP passes 0.65 threshold.\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_test_macro_f1(model_class, model_path, test_loader, device):\n",
    "    # Recreate model and load best weights\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(X)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(y.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    return macro_f1\n",
    "\n",
    "\n",
    "test_macro_f1 = evaluate_test_macro_f1(\n",
    "    model_class=MLPClassifier,\n",
    "    model_path=\"best_mlp_fasttext.pt\",\n",
    "    test_loader=test_loader,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Test Macro-F1:\", test_macro_f1)\n",
    "if test_macro_f1 > 0.65:\n",
    "    print(\"MLP passes 0.65 threshold.\")\n",
    "else:\n",
    "    print(\"MLP does not pass 0.65 threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17ae2523",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor for argument input is on cpu but expected on mps",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m test_loader:   \u001b[38;5;66;03m# or however you're batching test data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m         preds = torch.argmax(outputs, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     11\u001b[39m         all_preds.extend(preds.cpu().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat359/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat359/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mMLPClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat359/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat359/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat359/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat359/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat359/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stat359/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Tensor for argument input is on cpu but expected on mps"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:   # or however you're batching test data\n",
    "        outputs = model(xb)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"MLP Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat359-su25-py3.12 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
